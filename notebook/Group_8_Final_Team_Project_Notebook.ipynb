{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f919b1ef",
   "metadata": {},
   "source": [
    "# **Financial Risk Classification of S&P 500 Companies Using Machine Learning**\n",
    "\n",
    "## **Objective**\n",
    "apply supervised machine learning techniques to classify S&P 500 companies based on financial health indicators such as profit margins, debt levels, and return on equity. We aim to build a predictive model that categorizes companies as low, medium, or high financial risk. This can assist in investment decision-making and financial forecasting.\n",
    "\n",
    "## **Methodology**\n",
    "Our approach involves several key steps:\n",
    "1. Data preprocessing and exploration of variables\n",
    "2. Feature engineering to create meaningful predictors\n",
    "3. \n",
    "4. \n",
    "\n",
    "## **Data Overview**\n",
    "The dataset including numerous financial metrics that many professionals and investing gurus often use to value companies. This data is a look at the companies that comprise the S&P 500 (Standard & Poor's 500). The S&P 500 is a capitalization-weighted index of the top 500 publicly traded companies in the United States (top 500 meaning the companies with the largest market cap). The S&P 500 index is a useful index to study because it generally reflects the health of the overall U.S. stock market. The dataset was last updated in July 2020.\n",
    "\n",
    "### **Data Source**\n",
    "[S&P 500 Companies with Financial Information](https://www.kaggle.com/datasets/paytonfisher/sp-500-companies-with-financial-information?resource=download)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087eeee3",
   "metadata": {},
   "source": [
    "### 1. Install Dependencies\n",
    "In this section, we install all required dependencies listed in requirements.txt. These packages are essential for data processing, visualization, and implementing various machine learning algorithms for our wildfire risk prediction model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd92eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all required dependencies listed in requirements.txt\n",
    "# %pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291f81f7",
   "metadata": {},
   "source": [
    "### 2. Setup and Dependencies\n",
    "Here, we import all necessary Python libraries for:\n",
    "- Data manipulation (pandas, numpy)\n",
    "- Visualization (matplotlib, seaborn)\n",
    "- Statistical analysis\n",
    "- Machine learning models (scikit-learn, PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca643c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Library\n",
    "import math\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# Data Manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Statistics & Diagnostics\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Machine Learning: Preprocessing, Metrics, Utilities\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, PolynomialFeatures\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    mean_absolute_error, mean_squared_error, r2_score,\n",
    "    classification_report, confusion_matrix, ConfusionMatrixDisplay,\n",
    "    roc_curve, auc, precision_recall_curve\n",
    ")\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Machine Learning: Models\n",
    "# Linear Models\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "# Tree-based Models\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, GradientBoostingClassifier,\n",
    "    BaggingClassifier, AdaBoostClassifier\n",
    ")\n",
    "# Other Models\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Deep Learning with PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.float_format\", \"{:.2f}\".format)\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5c7fec",
   "metadata": {},
   "source": [
    "### 3. Load & Inspect Data\n",
    "This section loads the weather dataset from a CSV file. We then inspect the data structure, looking at the first few rows, data types, and checking for missing values. This step is crucial for understanding the dataset structure and quality before proceeding with analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bc0125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data_file = os.path.join(\"..\", \"dataset\", \"../dataset/financials.csv\")\n",
    "financial_data = pd.read_csv(data_file)\n",
    "\n",
    "# Display first few rows of the dataset\n",
    "# display(financial_data.head())\n",
    "print(financial_data.head()) \n",
    "\n",
    "# Display the data types of the columns\n",
    "print(\"\\nData types:\")\n",
    "print(financial_data.dtypes)\n",
    "\n",
    "# Check missing values\n",
    "print(\"\\nMissing values:\")\n",
    "print(financial_data.isnull().sum())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
