{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f919b1ef",
   "metadata": {},
   "source": [
    "# **Financial Risk Classification of S&P 500 Companies Using Machine Learning**\n",
    "\n",
    "## **Objective**\n",
    "Apply supervised machine learning techniques to classify S&P 500 companies based on financial health indicators such as profit margins, debt levels, and return on equity. We aim to build a predictive model that categorizes companies as low, medium, or high financial risk. This can assist in investment decision-making and financial forecasting.\n",
    "\n",
    "## **Methodology**\n",
    "Our approach involves several key steps:\n",
    "1. Load & clean financial data\n",
    "2. Engineer risk targets:\n",
    "   - Quantile-based (data-driven)\n",
    "   - Rule-based (expert-driven)\n",
    "3. Perform EDA and feature selection\n",
    "4. Train classifiers (Logistic Regression, Random Forest, XGBoost)\n",
    "5. Tune models and evaluate performance\n",
    "6. Compare classification strategies\n",
    "7. Perform regression (Linear, RF, XGBoost) to predict stock price\n",
    "\n",
    "## **Data Overview**\n",
    "The dataset including numerous financial metrics that many professionals and investing gurus often use to value companies. This data is a look at the companies that comprise the S&P 500 (Standard & Poor's 500). The S&P 500 is a capitalization-weighted index of the top 500 publicly traded companies in the United States (top 500 meaning the companies with the largest market cap). The S&P 500 index is a useful index to study because it generally reflects the health of the overall U.S. stock market. The dataset was last updated in July 2020.\n",
    "* Size: ~503 companies × ~25 variables\n",
    "* Features: Includes metrics such as:\n",
    "* P/E Ratio, Market Cap, Beta, EPS, Profit Margin, Debt/Equity, ROE, etc.\n",
    "* Format: Clean, structured CSV (filename: financials.csv)\n",
    "\n",
    "### **Data Source**\n",
    "[S&P 500 Companies with Financial Information](https://www.kaggle.com/datasets/paytonfisher/sp-500-companies-with-financial-information?resource=download)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087eeee3",
   "metadata": {},
   "source": [
    "### 1. Install Dependencies\n",
    "In this section, we install all required dependencies listed in requirements.txt. These packages are essential for data processing, visualization, and implementing various machine learning algorithms for our wildfire risk prediction model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd92eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all required dependencies listed in requirements.txt\n",
    "# %pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291f81f7",
   "metadata": {},
   "source": [
    "### 2. Setup and Dependencies\n",
    "Here, we import all necessary Python libraries for:\n",
    "- Data manipulation (pandas, numpy)\n",
    "- Visualization (matplotlib, seaborn)\n",
    "- Statistical analysis\n",
    "- Machine learning models (scikit-learn, PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca643c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Library\n",
    "import math\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# Data Manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Statistics & Diagnostics\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Machine Learning: Preprocessing, Metrics, Utilities\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, PolynomialFeatures\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    mean_absolute_error, mean_squared_error, r2_score,\n",
    "    classification_report, confusion_matrix, ConfusionMatrixDisplay,\n",
    "    roc_curve, auc, precision_recall_curve\n",
    ")\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Machine Learning: Models\n",
    "# Linear Models\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, RandomForestRegressor\n",
    "# Tree-based Models\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, GradientBoostingClassifier,\n",
    "    BaggingClassifier, AdaBoostClassifier\n",
    ")\n",
    "# Other Models\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "import xgboost as xgb\n",
    "\n",
    "# Deep Learning with PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.float_format\", \"{:.2f}\".format)\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5c7fec",
   "metadata": {},
   "source": [
    "### 3. Load & Inspect & Preprocess Data\n",
    "This section loads the weather dataset from a CSV file. We then inspect the data structure, looking at the first few rows, data types, and checking for missing values. This step is crucial for understanding the dataset structure and quality before proceeding with analysis.\n",
    "Also standardize column names to lowercase with underscores for consistency and ease of use in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bc0125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data_file = os.path.join(\"..\", \"dataset\", \"../dataset/financials.csv\")\n",
    "financial_df = pd.read_csv(data_file)\n",
    "\n",
    "# Display first few rows of the dataset\n",
    "# display(financial_df.head())\n",
    "print(financial_df.head()) \n",
    "\n",
    "# Display the data types of the columns\n",
    "print(\"\\nData types:\")\n",
    "print(financial_df.dtypes)\n",
    "\n",
    "# Check missing values\n",
    "print(\"\\nMissing values:\")\n",
    "print(financial_df.isnull().sum())\n",
    "\n",
    "print(financial_df.columns.tolist())\n",
    "\n",
    "# Rename columns to lowercase with underscores\n",
    "financial_df.columns = financial_df.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('/', '_')\n",
    "\n",
    "# Drop rows with missing values\n",
    "financial_df = financial_df.dropna()\n",
    "\n",
    "# safety copy\n",
    "df_quantile = financial_df.copy()\n",
    "df_rule = financial_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c5120f",
   "metadata": {},
   "source": [
    "### 4. Risk Target Engineering\n",
    "**Engineer a classification target variable (`risk_class`)** to group companies based on their financial health.\n",
    "  - We use **`earnings/share`** as a proxy for profitability.\n",
    "  - Companies are split into **Low**, **Medium**, and **High risk** groups using quantiles:\n",
    "    - **Low Risk**: Top third of companies with highest earnings/share  \n",
    "    - **Medium Risk**: Middle third  \n",
    "    - **High Risk**: Bottom third (lowest earnings/share)\n",
    "\n",
    "This allows us to transform the problem into a **supervised classification task** aligned with our project goal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaebfb90",
   "metadata": {},
   "source": [
    "#### 4.1. Quantile-Based Classification (Data-Driven)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d37a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantile-based\n",
    "# Sample rule: create a synthetic risk class from earnings/share (customize as needed)\n",
    "# High risk = low earnings\n",
    "df_quantile['risk_class'] = pd.qcut(df_quantile['earnings_share'], q=3, labels=['High', 'Medium', 'Low'])\n",
    "\n",
    "# Preview\n",
    "df_quantile[['name', 'earnings_share', 'risk_class']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39da8da",
   "metadata": {},
   "source": [
    "#### 4.2.  Rule-Based Classification (Expert-Driven)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39198842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rule-based risk classification\n",
    "def classify_risk(row):\n",
    "    if row['Earnings/Share'] < 1 or row['Price/Earnings'] > 40 or row['Dividend Yield'] < 0.01:\n",
    "        return 'High'\n",
    "    elif row['Earnings/Share'] < 3 or row['Price/Earnings'] > 25:\n",
    "        return 'Medium'\n",
    "    else:\n",
    "        return 'Low'\n",
    "\n",
    "df_rule['risk'] = df_rule.apply(classify_risk, axis=1)\n",
    "\n",
    "# Preview   \n",
    "df_rule[['name', 'Earnings/Share', 'Price/Earnings', 'Dividend Yield', 'risk']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abce7957",
   "metadata": {},
   "source": [
    "### 5. Exploratory Data Analysis (EDA)\n",
    "Below we explore the distribution, outliers, and relationships between financial metrics and the risk classes.\n",
    "* Histograms or boxplots of key financial variables\n",
    "* Correlation heatmap\n",
    "* Summary statistics\n",
    "* Target distribution (Low, Medium, High risk classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7584312c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric columns for visualization\n",
    "num_cols = ['price_earnings', 'dividend_yield', 'earnings_share', 'market_cap',\n",
    "            'ebitda', 'price_sales', 'price_book']\n",
    "\n",
    "# Histograms\n",
    "df_quantile[num_cols].hist(bins=20, figsize=(14, 10), color='skyblue', edgecolor='black')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correlation heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(df_quantile[num_cols].corr(), annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.show()\n",
    "\n",
    "# Risk class distribution\n",
    "sns.countplot(x='risk_class', data=df_quantile, order=['Low', 'Medium', 'High'])\n",
    "plt.title(\"Distribution of Risk Classes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601c902d",
   "metadata": {},
   "source": [
    "### 6. Feature Selection\n",
    "We use both domain knowledge and statistical methods (e.g., mutual information, feature importance from models) to identify the most relevant predictors.\n",
    "* Use correlation with target (Risk_Class)\n",
    "* Apply feature importance from tree models or mutual information scores\n",
    "* Drop irrelevant or redundant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a6ba28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode target\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df_quantile['risk_class'])\n",
    "X = df_quantile[num_cols].fillna(df_quantile[num_cols].median(numeric_only=True))\n",
    "\n",
    "# Simple imputation (replace NaNs with median of each column)\n",
    "X = X.fillna(X.median(numeric_only=True))  \n",
    "\n",
    "# Mutual Information scores\n",
    "mi_scores = mutual_info_classif(X, y)\n",
    "mi_series = pd.Series(mi_scores, index=X.columns).sort_values(ascending=False)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "mi_series.plot(kind='bar', color='green')\n",
    "plt.title(\"Mutual Information Scores for Predicting Financial Risk\")\n",
    "plt.ylabel(\"Mutual Information Score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dcc646",
   "metadata": {},
   "source": [
    "### 7. Train-Test Split\n",
    "We split the data into training and testing sets to evaluate model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b5a3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data (70% train, 30% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Train samples: {X_train.shape[0]}\")\n",
    "print(f\"Test samples: {X_test.shape[0]}\")\n",
    "\n",
    "# Check class distribution\n",
    "print(\"\\nTrain class distribution:\")\n",
    "print(pd.Series(y_train).value_counts())\n",
    "\n",
    "print(\"\\nTest class distribution:\")\n",
    "print(pd.Series(y_test).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2e80dd",
   "metadata": {},
   "source": [
    "### 8. Modeling – Try Multiple Classifiers (Quantile-Based)\n",
    "Train and compare models:\n",
    "* Logistic Regression (baseline)\n",
    "* Random Forest\n",
    "* XGBoost or Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6ff674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "lr_model = LogisticRegression(max_iter=1000)\n",
    "lr_model.fit(X_train, y_train)\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "\n",
    "print(\"Logistic Regression Performance:\\n\")\n",
    "print(classification_report(y_test, y_pred_lr, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00276f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "print(\"Random Forest Performance:\\n\")\n",
    "print(classification_report(y_test, y_pred_rf, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1b118b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost Classifier\n",
    "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "print(\"XGBoost Performance:\\n\")\n",
    "print(classification_report(y_test, y_pred_xgb, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a37a90b",
   "metadata": {},
   "source": [
    "### 9. Confusion Matrices (Quantile-Based)\n",
    "Visualize model predictions using confusion matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb5cabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Logistic Regression\": y_pred_lr,\n",
    "    \"Random Forest\": y_pred_rf,\n",
    "    \"XGBoost\": y_pred_xgb\n",
    "}\n",
    "\n",
    "for name, preds in models.items():\n",
    "    disp = ConfusionMatrixDisplay.from_predictions(\n",
    "        y_test, preds, display_labels=le.classes_, cmap=\"Blues\", values_format='d'\n",
    "    )\n",
    "    disp.ax_.set_title(f\"{name} - Confusion Matrix\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166fac3f",
   "metadata": {},
   "source": [
    "### 10. Model Comparison (Quantile-Based)\n",
    "We compare the models (Logistic Regression, Random Forest, XGBoost) using accuracy, F1 score, and macro-averaged metrics for a fair comparison across the multiclass setting.\n",
    "* Create a Comparison Table\n",
    "* Plot the Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad0dae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(name, y_true, y_pred):\n",
    "    return {\n",
    "        \"Model\": name,\n",
    "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"F1 Score (macro)\": f1_score(y_true, y_pred, average='macro'),\n",
    "        \"F1 Score (weighted)\": f1_score(y_true, y_pred, average='weighted')\n",
    "    }\n",
    "\n",
    "results = []\n",
    "results.append(evaluate_model(\"Logistic Regression\", y_test, y_pred_lr))\n",
    "results.append(evaluate_model(\"Random Forest\", y_test, y_pred_rf))\n",
    "results.append(evaluate_model(\"XGBoost\", y_test, y_pred_xgb))\n",
    "\n",
    "comparison_df = pd.DataFrame(results).sort_values(by=\"F1 Score (macro)\", ascending=False)\n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17205e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(data=comparison_df.melt(id_vars=\"Model\", var_name=\"Metric\", value_name=\"Score\"),\n",
    "            x=\"Model\", y=\"Score\", hue=\"Metric\")\n",
    "plt.title(\"Model Comparison\")\n",
    "plt.ylim(0, 1.05)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb211a3",
   "metadata": {},
   "source": [
    "### 11. Model Tuning (Grid Search for Random Forest & XGBoost)\n",
    "We'll tune hyperparameters for the top models using GridSearchCV and evaluate the best ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e10d220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Tuning\n",
    "\n",
    "rf_params = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 5, 10],\n",
    "    'min_samples_split': [2, 5],\n",
    "}\n",
    "\n",
    "rf_grid = GridSearchCV(RandomForestClassifier(random_state=42),\n",
    "                       rf_params, cv=3, scoring='f1_macro', n_jobs=-1)\n",
    "rf_grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Random Forest Params:\", rf_grid.best_params_)\n",
    "y_pred_rf_tuned = rf_grid.predict(X_test)\n",
    "print(\"Tuned Random Forest Performance:\\n\")\n",
    "print(classification_report(y_test, y_pred_rf_tuned, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0794180a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost Tuning\n",
    "xgb_params = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.3],\n",
    "}\n",
    "\n",
    "xgb_grid = GridSearchCV(XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42),\n",
    "                        xgb_params, cv=3, scoring='f1_macro', n_jobs=-1)\n",
    "xgb_grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best XGBoost Params:\", xgb_grid.best_params_)\n",
    "y_pred_xgb_tuned = xgb_grid.predict(X_test)\n",
    "print(\"Tuned XGBoost Performance:\\n\")\n",
    "print(classification_report(y_test, y_pred_xgb_tuned, target_names=le.classes_))\n",
    "\n",
    "results.append(evaluate_model(\"Tuned Random Forest\", y_test, y_pred_rf_tuned))\n",
    "results.append(evaluate_model(\"Tuned XGBoost\", y_test, y_pred_xgb_tuned))\n",
    "\n",
    "comparison_df = pd.DataFrame(results).sort_values(by=\"F1 Score (macro)\", ascending=False)\n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22681eaa",
   "metadata": {},
   "source": [
    "### Prepare Features and Train-Test Split for Rule-Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f7759f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "df_rule['risk_label'] = le.fit_transform(df_rule['risk'])\n",
    "\n",
    "# Drop unused or non-numeric columns\n",
    "X = df_rule.drop(columns=['Price', 'risk', 'risk_label', 'Name', 'Symbol', 'SEC Filings'], errors='ignore')\n",
    "X = X.select_dtypes(include=[np.number])  # Numeric features only\n",
    "y = df_rule['risk_label']\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b72bfb",
   "metadata": {},
   "source": [
    "#### Train Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e5c94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "y_pred_rf = rf_clf.predict(X_test)\n",
    "\n",
    "print(\"Random Forest Classification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred_rf, target_names=le.classes_))\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(rf_clf, X_test, y_test, display_labels=le.classes_, cmap='Blues')\n",
    "plt.title(\"Random Forest - Rule-Based Risk\")\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6ddc22",
   "metadata": {},
   "source": [
    "#### Train XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30b0fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_clf = XGBClassifier(n_estimators=100, learning_rate=0.1, eval_metric='mlogloss', random_state=42)\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "y_pred_xgb = xgb_clf.predict(X_test)\n",
    "\n",
    "print(\"XGBoost Classification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred_xgb, target_names=le.classes_))\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(xgb_clf, X_test, y_test, display_labels=le.classes_, cmap='Greens')\n",
    "plt.title(\"XGBoost - Rule-Based Risk\")\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f26be91",
   "metadata": {},
   "source": [
    "## 12. Comparison of Risk Classification Strategies\n",
    "\n",
    "In this section, we compare two strategies for labeling financial risk:\n",
    "\n",
    "### 1. Quantile-Based Risk (Data-Driven)\n",
    "- Labels companies into *High*, *Medium*, and *Low* risk based on **earnings per share** (EPS) using quantile cuts.\n",
    "- **Advantage**: Unbiased and purely based on data distribution.\n",
    "- **Disadvantage**: Ignores domain knowledge or business context.\n",
    "\n",
    "### 2. Rule-Based Risk (Expert-Driven)\n",
    "- Uses explicit thresholds for **EPS**, **P/E ratio**, and **Dividend Yield** to determine risk.\n",
    "- **Advantage**: Mimics analyst/business logic.\n",
    "- **Disadvantage**: Fixed and may not generalize well to new data.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔄 Labeling Criteria Comparison\n",
    "\n",
    "| Feature            | Quantile-Based                            | Rule-Based                                                    |\n",
    "|-------------------|--------------------------------------------|----------------------------------------------------------------|\n",
    "| `earnings/share`  | Divided into 3 quantiles                   | `< 1` → High, `< 3` → Medium, else Low                        |\n",
    "| `price/earnings`  | Not used                                   | `> 40` → High, `> 25` → Medium                                |\n",
    "| `dividend_yield`  | Not used                                   | `< 0.01` → High                                               |\n",
    "| Adaptability      | ✅ Flexible, data-driven                   | ❌ Fixed thresholds                                           |\n",
    "| Risk Class Balance| ✅ Balanced (33% each by design)           | ❌ May be imbalanced depending on data                        |\n",
    "\n",
    "---\n",
    "\n",
    "### 📊 Performance Comparison\n",
    "\n",
    "We evaluated both risk-labeling strategies using **Random Forest** and **XGBoost** classifiers.\n",
    "\n",
    "| Model            | Label Type     | Accuracy | F1 Score (macro) | Notes                          |\n",
    "|------------------|----------------|----------|------------------|---------------------------------|\n",
    "| Random Forest    | Quantile-Based | ~98%     | ~0.98            | Balanced across all classes     |\n",
    "| XGBoost          | Quantile-Based | ~97%     | ~0.97            | Slight drop on High-risk class  |\n",
    "| Random Forest    | Rule-Based     | ~98%     | ~0.98            | High precision & recall         |\n",
    "| XGBoost          | Rule-Based     | ~97%     | ~0.97            | Consistent performance           |\n",
    "\n",
    "> *Replace \"~\" with actual numbers from your output.*\n",
    "\n",
    "---\n",
    "\n",
    "### 📉 Confusion Matrix Highlights\n",
    "\n",
    "- **Quantile-based:** Ensures evenly distributed classes and performs well across them.\n",
    "- **Rule-based:** May result in class imbalance depending on how strict the thresholds are, especially for *High-risk*.\n",
    "\n",
    "(Visual confusion matrices are shown in earlier sections.)\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Final Thoughts\n",
    "\n",
    "- **Quantile-based labeling** is ideal when you want the model to learn from data distributions.\n",
    "- **Rule-based labeling** offers interpretability and aligns with expert-defined heuristics.\n",
    "- **Combining both approaches** in your analysis shows robustness and enhances the credibility of your classification pipeline.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbd4c1b",
   "metadata": {},
   "source": [
    "## Optional Extension – Predicting Stock Price Using Regression\n",
    "\n",
    "\"Can we both classify financial risk and predict stock price from public metrics?\"\n",
    "\n",
    "In this optional extension, we shift our focus from classifying companies into risk categories to predicting their stock price (`Price`) directly from financial metrics using regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68647488",
   "metadata": {},
   "source": [
    "### Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a6009c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop non-numeric/categorical columns\n",
    "df = financial_df.drop(columns=['symbol', 'name', 'sec_filings', 'risk_class'])\n",
    "df = df.dropna()\n",
    "df = pd.get_dummies(df, columns=['sector'], drop_first=True)\n",
    "\n",
    "# Define features and target\n",
    "X = df.drop(columns=['price'])\n",
    "y = df['price']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Linear Regression\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = lr_model.predict(X_test)\n",
    "lr_rmse = mean_squared_error(y_test, y_pred) ** 0.5\n",
    "lr_r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Linear Regression RMSE:\", lr_rmse)\n",
    "print(\"Linear Regression R²:\", lr_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1c17c4",
   "metadata": {},
   "source": [
    "### Linear Regression Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd7a32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(y_test, y_pred, alpha=0.6)\n",
    "plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--')\n",
    "plt.xlabel(\"Actual Price\")\n",
    "plt.ylabel(\"Predicted Price\")\n",
    "plt.title(\"Linear Regression: Actual vs Predicted\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2583ad4e",
   "metadata": {},
   "source": [
    "### Random Forest Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a32773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the cleaned and encoded dataframe from the previous regression section\n",
    "# Drop irrelevant columns and encode categorical variable\n",
    "df_rf = financial_df.drop(columns=['symbol', 'name', 'sec_filings', 'risk_class'])\n",
    "df_rf = df_rf.dropna()\n",
    "df_rf = pd.get_dummies(df_rf, columns=['sector'], drop_first=True)\n",
    "\n",
    "# Features and target\n",
    "X = df_rf.drop(columns=['price'])\n",
    "y = df_rf['price']\n",
    "\n",
    "# Split into train/test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Random Forest Regressor\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = rf_model.predict(X_test)\n",
    "rf_rmse = mean_squared_error(y_test, y_pred) ** 0.5\n",
    "rf_r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Random Forest Regression RMSE:\", rf_rmse)\n",
    "print(\"Random Forest Regression R²:\", rf_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9318afb3",
   "metadata": {},
   "source": [
    "### Random Forest Regression Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccec2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(y_test, y_pred, alpha=0.6)\n",
    "plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--')  # reference line\n",
    "plt.xlabel(\"Actual Price\")\n",
    "plt.ylabel(\"Predicted Price\")\n",
    "plt.title(\"Random Forest Regression: Actual vs Predicted\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71df9e0",
   "metadata": {},
   "source": [
    "### XGBoost Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dccd720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Data\n",
    "\n",
    "# Use the same preprocessed data\n",
    "df_xgb = financial_df.drop(columns=['symbol', 'name', 'sec_filings', 'risk_class'])\n",
    "df_xgb = df_xgb.dropna()\n",
    "df_xgb = pd.get_dummies(df_xgb, columns=['sector'], drop_first=True)\n",
    "\n",
    "# Features and target\n",
    "X = df_xgb.drop(columns=['price'])\n",
    "y = df_xgb['price']\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train XGBoost Regressor\n",
    "xgb_model = XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "xgb_rmse = mean_squared_error(y_test, y_pred) ** 0.5\n",
    "xgb_r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"XGBoost Regression RMSE:\", xgb_rmse)\n",
    "print(\"XGBoost Regression R²:\", xgb_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebeb388b",
   "metadata": {},
   "source": [
    "### XGBoost Feature Importance Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df062627",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = xgb_model.feature_importances_\n",
    "feature_names = X.columns\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(importances)), importances[indices], align='center')\n",
    "plt.xticks(range(len(importances)), feature_names[indices], rotation=90)\n",
    "plt.title(\"XGBoost Feature Importance\")\n",
    "plt.tight_layout()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4671a4",
   "metadata": {},
   "source": [
    "### Actual vs Predicted Plot – XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbbb1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(y_test, y_pred, alpha=0.6)\n",
    "plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--')\n",
    "plt.xlabel(\"Actual Price\")\n",
    "plt.ylabel(\"Predicted Price\")\n",
    "plt.title(\"XGBoost Regression: Actual vs Predicted\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fd9b65",
   "metadata": {},
   "source": [
    "### Summary Table for All Regression Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d518a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_results = pd.DataFrame({\n",
    "    \"Model\": [\"Linear Regression\", \"Random Forest\", \"XGBoost\"],\n",
    "    \"RMSE\": [lr_rmse, rf_rmse, xgb_rmse],\n",
    "    \"R² Score\": [lr_r2, rf_r2, xgb_r2]\n",
    "})\n",
    "\n",
    "print(regression_results.sort_values(by=\"R² Score\", ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e811259",
   "metadata": {},
   "source": [
    "### 12. Deployment and Discussion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
